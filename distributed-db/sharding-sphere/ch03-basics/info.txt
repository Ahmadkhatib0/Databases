It is difficult to meet massive data in terms of its performance, availability, operation, and maintenance cost: 
• Performance: Relational databases mostly use B+ tree indexes. When the amount of data is too large, the increase 
  in the index depth will increase the amount of disk access I/O, resulting in the decline of query performance.
• Availability: As application services are stateless, they can realize low-cost local capacity expansion making 
  the database become the bottleneck of the whole system. It has become increasingly difficult for traditional 
  single data nodes or primary- secondary architectures to bear the pressure of the whole system. For these reasons,
  databases' availability has become increasingly important, to the point of becoming any given system's key.
• Operation and maintenance costs: When the amount of data in the database instance increases to a certain 
  level, the operation and maintenance costs will greatly increase. The cost in terms of time lost for data 
  backup and recovery will eventually grow exponentially, and in a way that is directly proportional to the
  amount of data being managed. In some cases, when relational databases cannot meet the storage and access 
  requirements of massive data, some users store data in the original NoSQL that supports distribution. However, 
  NoSQL has incompatibility problems with SQL and imperfect support for transactions, so it cannot completely
  replace the relational database – and the core position of the relational database is still unshakable.
  
Data sharding refers to splitting the data in a single database into multiple databases or
  tables according to a certain dimension, to improve performance and availability.
  Data sharding is not to be confused with data partitioning, which is about dividing the
  data into sub-groups while keeping it stored in a single database. Many other opinions and
  ideas are floating around in academia and on the internet about this, but rest assured that
  the number of databases where the data is stored represents the main difference that you
  should be aware of when distinguishing between sharding and partitioning.

we can divide data sharding into two common forms – database shards and table shards:
• Database shards are partitions of data in a database, with each shard being stored in a different database instance.
• Table shards are the smaller pieces that used to be part of a single table and 
  are now spread across multiple databases.

According to the data sharding methodology, we can divide data sharding into vertical
 sharding and horizontal sharding: 

Vertical sharding refers to splitting the data according to your requirements, and its core concept is 
  dedicated to special databases. Before splitting, a database consists of multiple tables, and each table 
  corresponds to different businesses. After splitting, tables are classified according to the business and 
  distributed to different databases. This helps disperse the access pressure to different databases.

Horizontal sharding refers to dispersing data into multiple databases or tables according to certain 
  rules through a field (or several fields). Here, each shard contains only part of the data
  Although data sharding solves performance, availability, operation, and maintenance
  issues, it also introduces new problems. After sharding data, it becomes very difficult for application 
  development engineers and database administrators to operate the database if they wish to update, move,
  edit, or reconfigure the data. At the same time, many SQLs that can run correctly in the single-node 
  database may not run correctly in the database after sharding.



Data sharding key points: 
  To reduce the cost of using the sharding function and realize data sharding transparency,
  ShardingSphere has introduced some core concepts, including table, data node, sharding,
  row expression, and distributed primary key: 
Table
Table is the key concept surrounding data sharding. ShardingSphere provides a variety
  of table types, including logical table, real table, binding table, broadcast table, and single
  table, to meet the needs of data sharding in different scenarios:
  
• Logical table: This refers to the logical name of the horizontal split database (table) with the same 
  structure and is the logical identification of the table in SQL. For example, if the order data is divided 
  into 20 tables according to the primary key's module, which are t_order_0 to t_order_19, respectively, 
  then the logical table name of the order table is t_order.
  
• Real table: The real table refers to the physical table that exists in the horizontally split database 
  – that is, the logical representation of t_order_0 to t_order_9. 
  
. Binding table: A binding table refers to the main and sub-tables with consistent sharding rules. 
  Remember that you must utilize the sharding key to associate multiple tables with a query. Otherwise, Cartesian 
  product association or cross- database association will occur, thus affecting your query efficiency.
  
. Broadcast table: The table that exists in all sharding data sources is called the broadcast table. The 
  table's structure and its data are completely consistent in each database. Broadcast tables are suitable for 
  scenarios where the amount of data is small and needs to be associated with massive data tables, such as 
  dictionary tables.
  
. Single table: On the other hand, a table that only exists in all of the sharding data
  sources is called a single table. Single tables apply to tables with a small amount of
  data and without sharding.

Data node
  Data node is the smallest unit of data sharding. It is the mapping relationship between the logical table 
  and the real table. It is composed of the data source's name and the real table. When configuring multiple 
  data nodes, they need to be separated by commas; for example, ds_0.t_order_0, ds_0.t_order_1, 
  ds_1.t_order_0, ds_1.t_order_1. Users can configure the nodes freely according to their needs.


Sharding
Sharding mainly includes the core concepts of sharding key, sharding algorithm, and sharding strategy :
  • The sharding key refers to the database field that's used to split the database (table) horizontally. 
    For example, in an order table, if you want to modify the significant figure of its primary key, 
    the order primary key becomes the shard field. ShardingSphere's data sharding function supports 
    sharding according to single or multiple fields.
  • The sharding algorithm refers to the algorithm that's used to shard data. It supports =, > =, < =, >, <, 
    between, and in for sharding. The sharding algorithm can be implemented by developers, or the built-in 
    sharding algorithm syntax of Apache ShardingSphere can be used, with high flexibility. 
  • The sharding strategy includes the sharding key and sharding algorithm. It is the real object for the sharding 
    operation. Because the sharding algorithm is independent, the sharding algorithm is extracted independently.
  
Row expression: 
  To simplify and integrate the configuration, ShardingSphere provides row expressions to simplify the 
  configuration workload of data nodes and sharding algorithms. Using row expressions is very simple. You 
  only need to use ${ expression } or $->{ expression } to identify row expressions in the configuration. 
  The row expression uses Groovy syntax. It is derived from Apache Groovy, an object-oriented programming
  language that was made for the Java platform and is Java-syntax-compatible. All operations that Groovy 
  supports can be supported by row expression. Regarding the previous data node example, where we have 
  ds_0.t_order_0, ds_0.t_order_1, ds_1.t_order_0, ds_1.t_order_1, the row expression can be simplified to 
  db${0..1}.t_order${0..1} or db$->{0..1}.t_order$->{0..1}.


Sharding workflow: 
SQL Parser is responsible for parsing the original user SQL, which can be divided into lexical analysis 
  and syntax analysis. Lexical analysis is responsible for splitting SQL statements into non-separable words; 
  then, the parser, through syntax analysis, understands SQL and obtains the SQL Statement. You can think 
  of this process as involving lexical analysis, followed by grammar analysis.
  
The SQL Statement includes a table, selection item, sorting item, grouping item, aggregating function, 
  paging information, query criteria, placeholder mark, and other information.
  
SQL Binder combines metadata and the SQL Statement to supplement wildcards and missing parts in SQL, 
  generate a complete parsing context that conforms to the database table structure, and judge whether 
  there are distributed queries across multiple data according to the context information. This helps you 
  decide whether to use the SQL Federation Engine. Now that you have a general understanding of the sharding 
  workflow, let's dive a little deeper into the Simple Push Down engine and SQL Federation Engine process.


Simple Push Down engine: 
  The Simple Push Down engine includes processes such as SQL Parser, SQL Binder, SQL Router, SQL Rewriter, 
  SQL Executor, and Result Merger, which are used to process SQL execution in standard sharding scenarios.

SQL Router matches the sharding strategy of the database and table according to the parsing context and 
  generates the routing context. Apache ShardingSphere 5.0 supports sharding routing and broadcast routing. 
  SQL with a sharding key can be divided into single-chip routing, multi-chip routing, and range routing
  according to the sharding key. SQL without a sharding key adopts broadcast routing.

According to the routing context, SQL Rewriter is responsible for rewriting the logical SQL that's written 
  by the user into real SQL, which can be executed correctly in the database. SQL rewriting can be divided 
  into correctness rewriting and optimization rewriting. Correctness rewriting includes rewriting the logical 
  table name in the table shard's configuration to the real table name after routing, column supplement, and
  paging information correction. Optimization rewriting is an effective means of improving
  performance without affecting query correctness.

SQL Executor is responsible for sending the routed and rewritten real SQL to the underlying data source for 
  execution safely and efficiently through an automated execution engine. SQL Executor pays attention to 
  balancing the consumption that's caused by creating a data source connection and memory occupation. 
  It's expected to maximize the rational use of concurrency to the greatest extent and realize automatic
  resource control and execution efficiency.
  
Result Merger is responsible for combining the multiple result datasets that were obtained from various data 
  nodes into a result set and then correctly returning them to the requesting client. ShardingSphere supports 
  five merge types: traversal, sorting, grouping, paging, and aggregation. They can be combined rather than 
  being mutually exclusive. In terms of structure, it can be divided into stream, memory, and decorator merging.


SQL Federation Engine: 
  SQL Federation Engine is a crucial element not only in the implementation of data sharding but in the 
  overall ShardingSphere ecosystem. The SQL Federation Engine includes processes such as SQL Parser, 
  Abstract Syntax Tree (AST), SQL Binder, SQL Optimizer, Data Fetcher, and Operator Calculator, which are used 
  to process association queries and sub-queries across multiple database instances. The bottom layer uses 
  calculations to optimize the Rule-Based Optimizer (RBO) and Cost-Based Optimizer (CBO) based on relational 
  algebra, and query results through the optimal execution plan.

SQL Optimizer is responsible for optimizing the association query and sub-query across
  multiple database instances, as well as performing rule-based optimization and cost-based
  optimization to obtain the optimal execution plan.

Data Fetcher is responsible for obtaining data from the storage node according to the SQL that was generated 
  by the optimal execution plan. Data Fetcher also routes, rewrites, and executes the generated SQL.


To meet the associated queries and subqueries across multiple database instances in distributed scenarios, 
  ShardingSphere provides built-in SQL optimization functions through the federation execution engine. 
  This helps achieve the optimal performance of query statements in distributed scenarios.

• Logical optimization is RBO and refers to the equivalent transformation rules based on relational algebra 
  to optimize query SQL, including column clipping, predicate pushdown, and other optimization contents.
• Physical optimization is CBO and refers to optimizing query SQL based on cost, including 
  table connection mode, table connection order, sorting, and other optimization contents.

RBO
  RBO refers to rule-based optimization. The theoretical basis of RBO is relational algebra. It realizes the logical 
  optimization of SQL based on the equivalent transformation rules of relational algebra. It accepts a logical 
  relational algebraic expression before returning the logical relational expression after rule transformation.

CBO
  CBO refers to cost-based optimization. The SQL optimizer is responsible for estimating the cost of the query 
  according to the cost model so that you can select the execution plan with the lowest cost per query.



• Atomicity: All the operations in the transaction succeed or fail (be it read, write, update, or data deletion).
• Consistency: The status before and after the transaction's execution meets the same constraint. For example, 
  in the classic transfer business, the account sum of the two accounts is equal before and after the transfer.
• Isolation: When transactions are executed concurrently, isolation acts as concurrency control. It ensures that 
  the transaction's execution impacts the database in the same way as sequentially executed transactions would.
• Durability: By durability, we refer to the insurance that any changes that are made to the data by 
  successfully executed transactions will be saved, even in cases of system failure (such as a power outage).


Distributed transactions: 
  Based on the CAP and BASE theories, distributed transactions are generally divided into
  rigid transactions and flexible transactions:
  
• Rigid transactions represent the strong consistency of the data.

• Flexible transactions represent the final consistency of the data.
  The implementation method for distributed transactions has three roles:
  
• Application program (AP): This is an application that initiates logical transactions.

• Transaction manager (TM): A logical transaction will have multiple branch
  transactions to coordinate the execution, commit, and rollback of branch transactions.
  
• Resources manager (RM): This role executes branch transactions. Considering distributed 
  transactions' three roles, these are performed while following a two-phase submission:
    1. Preparation phase: 
      • TM informs RM to perform the relevant operations in advance. 
      • RM checks the environment, locks the relevant resources, and executes them. If the execution is successful, 
        it is in the prepared state and not in the completed state, and TM is notified of the execution result.
    2. Commit/rollback phase:
      • If TM receives all the results that are returned by RM as successful, it will notify RM to submit the transaction.
      • Otherwise, TM will notify RM to roll back.

Note: Saga transaction and two-phase submission are very similar. The difference is
  that in the preparation phase, if the execution is successful, there is no prepared state.


Local transaction
  A local transaction is based on the transaction of the underlying database. The distributed transaction 
  can support the submission of cross-database transactions and the rollback that's caused by logical errors. 
  Because there is no transaction maintaining the intermediate state, if there are network and hardware-related 
  exceptions during the execution of a transaction, data may be inconsistent.

XA transaction
  For XA transactions, two implementations of the open source transaction managers, atomic and Narayana, 
  are integrated to ensure maximum data protection and avoid corruption. Based on the two-phase submission 
  and the XA interface of the underlying database, you must maintain the transaction log in the intermediate state,
  which can support distributed transactions. At the same time, if problems are caused by unresponsive hardware 
  (such as in case of a power outage or crash), the network, and exceptions, the proxy can roll back or commit
  the transactions in the intermediate state according to the transaction log. Moreover, you can configure shared 
  storage to store transaction logs (such as using MySQL to store transaction logs). By deploying the cluster 
  mode of multiple proxies, you can increase the performance of proxies and support distributed transactions 
  that include multiple proxies.

Flexible transaction
  SEATA's Saga model transaction is integrated to provide a flexible transaction based on compensation. 
  If an exception or error occurs in each branch transaction, the global transaction manager performs the 
  opposite compensation operation and rolls back through the maintained transaction log to achieve the final 
  consistency. You have now reviewed the definitions and characteristics of all local, distributed, and flexible 
  transactions – but what are the differences between them? Could you pinpoint some key parameters to 
  differentiate them by? The next section will give you the knowledge to do just that.


 which transaction type will fit which scenario: 
• If the business can handle data inconsistencies caused by local transactions that have
  high-performance requirements, then local transaction mode is recommended.
  
• If strong data consistency and low concurrency are required, XA transaction mode is an ideal choice.

• If certain data consistency can be sacrificed, the transaction is large, and the
  concurrency is high, then flexible transaction mode is a good choice.

• Vertical scaling can be achieved by upgrading the single hardware. Affected by Moore's law, which states 
  that the number of transistors on a microchip doubles approximately every 2 years, as well as by hardware 
  costs, with you having to continuously upgrade single hardware, the marginal benefit of this scheme
  decreases. To solve this problem, the database industry has developed a horizontal scaling scheme.
• Horizontal scaling can be achieved by increasing or decreasing ordinary hardware resources. Although 
  horizontal scaling interests hardware, the current state of applications in the computing layer has become 
  relatively mature, and horizontal scaling can be well supported by the Share-Nothing architecture. 
  This architecture is a typical architecture for distributed computing since, thanks to its design, 
  each update is satisfied by a single node in the compute cluster.

Elastic scaling: is the ability to automatically and flexibly add, reduce, or even remove compute infrastructure 
  based on the changing requirement patterns dictated by traffic. On the data target side, elastic scaling 
  can be divided into migration scaling and autoscaling. If the target is a new cluster, it is migration 
  scaling. If the target end is a new node of the original cluster, it is auto-scaling

Apache ShardingSphere also provides good support in the following aspects:
• Operation convenience: Elastic scaling can be triggered by DistSQL, and the operation experience is the 
  same as that of SQL. DistSQL is an extended SQL designed by Apache ShardingSphere, which provides a unified 
  capability extension on the upper layer of a traditional database.
  
• Degree of freedom of the sharding algorithm: Different sharding algorithms have different characteristics. 
  Some are conducive to data range queries, while others are conducive to data redistribution. 
  Apache ShardingSphere supports rich types of sharding algorithms. In addition to modulus, hash, range, and 
  time, it also supports user-defined sharding algorithms.


Once elastic scaling has been triggered with ShardingSphere, the main workflow of the system will be as follows:
1. Stock data migration: The data originally located in the source storage cluster is stock data. 
   Stock data can be extracted directly and then imported into the target storage cluster efficiently.
2. Incremental data migration: When data is migrated, the system is still providing services, and new data 
   will enter the source storage cluster. This part of the new data is called incremental data and can be obtained 
   through change data capture (CDC) technology, and then imported into the target storage cluster efficiently.
3. Detection of an incremental data migration progress: Since incremental data changes dynamically, 
   we need to select the time point where there is no incremental data for subsequent steps to reduce the 
   impact on the existing system.
4. Read-only mode: Set the source storage cluster to read-only mode. Compare the data for consistency: 
   Compare whether the data in the target end and the source end are consistent. The current implementation is 
   at this stage and the new version will be optimized.
5. Target storage cluster: Switch configuration to the new target storage cluster by
   specifying the data source and rules.

Elastic scaling key points
  Scheduling is the basis of elastic scaling. Data migration tasks are divided into multiple
  parts for parallel execution, clustered migration, automatic recovery after migration
  exceptions, online encryption and desensitization, and process scheduling at the same
  time, which are supported by the scheduling system.

Apache ShardingSphere uses the following methods to ensure the correctness of data:
• Data consistency verification: This compares whether the data at the source end and the data at the target 
  end are consistent. If the data at both ends is inconsistent, the system will judge that elastic scaling 
  failed and will not switch to a new storage cluster, to ensure that bad data will not go online. The data 
  consistency verification algorithm supports SPI customization.
  
• The source storage cluster is set to read-only: The incremental data changes dynamically. To confirm 
  that the data at both ends is completely consistent, a certain write stop time is required to ensure 
  that the data does not change. The time taken to stop writing is generally determined by the amount of 
  data, the arrangement of the verification process, and the verification algorithm.

Apache ShardingSphere uses the following incremental functions to enable new storage clusters faster:
• The data migration task is divided into multiple parts and executed in parallel.
• The modification operations of the same record are merged 
  (for example, 10 update operations are merged into one update operation).
• We utilize a batch data import to improve speed by dividing the data into batches to not overload the system.
• We have a breakpoint resume transmission feature to ensure the continuity of data
  transfers in case of an unexpected interruption.
• Cluster migration. This function is under planning and hasn't been completed at the time of writing.


The pluggable model is divided into three layers: the
  L1 kernel layer, the L2 function layer, and the L3 ecological layer. Elastic scaling is also
  integrated into the three-layer pluggable model, which is roughly layered as follows:
  
• Scheduling: Located in the L1 kernel layer, this includes task scheduling and task arrangement. 
  It provides support for upper-layer functions such as elastic scaling, online encryption and 
  desensitization, and MGR detection, and will support more functions in the future.
  
• Data ingestion: Located in the L1 kernel layer, this includes stock data extraction
  and incremental data acquisition. It supports upper-layer functions such as elastic scaling 
  and online encryption and desensitization, with more features to be supported in the future.
  
• The core process of the data pipeline: Located in the L1 kernel layer, this includes data pipeline 
  metadata and reusable basic components in each step. It can be flexibly configured and assembled to 
  support upper-layer functions such as elastic scalability and online encryption and desensitization. 
  More features will be supported in the future.
  
• Elastic scaling, online encryption, and desensitization: Located in the L2 function
  layer, it reuses the L1 kernel layer and achieves lightweight functions through
  configuration and assembly. The SPI interface of some L1 kernel layers is realized
  through the dependency inversion principle.
  
• Implementation of database dialect: Located in the L3 ecological layer, it includes
  source end database permission checks, incremental data acquisition, data
  consistency verification, and SQL statement assembly.
  
• Data source abstraction and encapsulation: These are located in the L1 kernel layer and the L2 
  function layer. The basic classes and interfaces are located in the L1 kernel layer, while the 
  implementation based on the dependency inversion principle is located in the L2 function layer.

Note: Elastic scaling is not supported in the following events:
• If a database table does not have a primary key
• If the primary key of a database table is a composite primary key
• If there is no new database cluster at the target end


Key points regarding the read/write splitting function: 
•• Primary database refers to the database that's used for adding, updating, and
   deleting data. At the time of writing, only a single primary database is supported.
   
•• Secondary database refers to the database that's used for query data operation,
   which can support multiple secondary databases.
   
•• Primary-secondary synchronization refers to asynchronously synchronizing data from the primary database 
   to the secondary database. Due to the asynchrony of primary-secondary database synchronization, the data 
   of the secondary database and primary database will be inconsistent for a short time.
   
•• A load balancing strategy is used to divert query requests to different secondary databases.



